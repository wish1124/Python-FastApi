import numpy as np
import pandas as pd
import torch
import os
import matplotlib.pyplot as plt

# ê°™ì€ í´ë”ì— model_transformer.pyê°€ ìˆì–´ì•¼ í•¨
from model_transformer import run_training_transformer

def main():
    # ë°ì´í„° ê²½ë¡œ ì„¤ì • (ìƒëŒ€ ê²½ë¡œ ì£¼ì˜)
    csv_path = "../dataset/dataset_feature_selected.csv"
    
    if not os.path.exists(csv_path):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        # í˜¹ì‹œ í˜„ì¬ í´ë” ê¸°ì¤€ì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì²´í¬
        if os.path.exists("./dataset_feature_selected.csv"):
            csv_path = "./dataset_feature_selected.csv"
            print(f"ğŸ“‚ í˜„ì¬ í´ë”ì—ì„œ íŒŒì¼ ë°œê²¬: {csv_path}")
        else:
            return

    print(f"ğŸ“‚ Loading data from: {csv_path}")
    df = pd.read_csv(csv_path)

    # 1. íƒ€ê²Ÿ ë° ì‚¬ìš©í•  í•µì‹¬ í”¼ì²˜ 4ê°œ ì •ì˜
    target_col = "ë‚™ì°°ê°€"
    
    # [ìˆ˜ì •ë¨] ì „ì²´ ì»¬ëŸ¼ ìë™ ê°ì§€ ëŒ€ì‹  4ê°œ ì»¬ëŸ¼ ê³ ì •
    feature_cols = ["ê¸°ì´ˆê¸ˆì•¡", "ì¶”ì •ê°€ê²©", "ì˜ˆê°€ë²”ìœ„", "ë‚™ì°°í•˜í•œìœ¨"]

    # 2. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (íƒ€ê²Ÿ + í”¼ì²˜ê°€ ëª¨ë‘ ìˆëŠ”ì§€)
    required_cols = feature_cols + [target_col] 
    missing = [c for c in required_cols if c not in df.columns]
    
    if missing:
        raise KeyError(f"CSVì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

    print(f"ğŸ“Š ì‚¬ìš©ë  ì…ë ¥ í”¼ì²˜ ({len(feature_cols)}ê°œ): {feature_cols}")

    # 3. í•™ìŠµ ì‹¤í–‰
    print("ğŸš€ Transformer í•™ìŠµ ì‹œì‘ (4 Features)...")
    res = run_training_transformer(
        df=df,
        feature_cols=feature_cols,
        target_col=target_col,
        target_log=True,         # íƒ€ê²Ÿ ë¡œê·¸ ë³€í™˜ ì‚¬ìš©
        epochs=300,               
        patience=30,             # ì¸ë‚´ì‹¬ ì¦ê°€ (í•™ìŠµ ì•ˆì •í™” ê³ ë ¤)
        batch_size=128,          # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •
        lr=1e-4,                 # í•™ìŠµë¥ 
        weight_decay=1e-4,
        d_model=64,              # ì…ë ¥ ì°¨ì›ì´ ì‘ìœ¼ë¯€ë¡œ ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¶•ì†Œ (512 -> 64)
        nhead=4,
        num_layers=2,            # ë ˆì´ì–´ ìˆ˜ ì¶•ì†Œ (3 -> 2)
        dim_feedforward=256,     # FFN ì°¨ì› ì¶•ì†Œ (2048 -> 256)
        dropout=0.1,
        # model_transformer.py ì •ì˜ì— ë”°ë¼ ì•„ë˜ ì¸ìëŠ” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í•„ìš”
        # verbose=True 
    )

    print("\nâœ… Best VAL Loss:", res.best_val)
    print("âœ… TEST Loss:", res.test)

    # 4. ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    sample_size = min(5, len(df))
    sample = df.sample(sample_size, random_state=42).copy()

    # ë°ì´í„° ì „ì²˜ë¦¬ (Scaler ì‚¬ìš©)
    X = sample[feature_cols].to_numpy(dtype=np.float32)
    X_s = res.x_scaler.transform(X)

    # í…ì„œ ë³€í™˜ ë° 3D ë³€í™˜
    # (Batch, Features) -> (Batch, Features, 1)
    x_t = torch.from_numpy(X_s)
    x_t = x_t.unsqueeze(-1)  
    
    device = next(res.model.parameters()).device
    res.model.eval()
    
    with torch.no_grad():
        pred_s = res.model(x_t.to(device)).cpu().numpy()

    # ì—­ë³€í™˜ (Log -> ì›ë˜ ê°€ê²©)
    use_log = getattr(res, 'target_log', True)
    
    pred_log = res.y_scaler.inverse_transform(pred_s)
    pred_amt = np.expm1(pred_log) if use_log else pred_log

    # ê²°ê³¼ ë¹„êµ ì¶œë ¥
    out = sample[[target_col]].copy()
    out["ì˜ˆì¸¡ë‚™ì°°ê°€"] = pred_amt
    out["ì˜¤ì°¨(ì˜ˆì¸¡-ì‹¤ì œ)"] = out["ì˜ˆì¸¡ë‚™ì°°ê°€"] - out[target_col]
    
    # ì˜¤ì°¨ìœ¨ ê³„ì‚°
    out["ì˜¤ì°¨ìœ¨(%)"] = 0.0
    mask = out[target_col] != 0
    out.loc[mask, "ì˜¤ì°¨ìœ¨(%)"] = (out.loc[mask, "ì˜¤ì°¨(ì˜ˆì¸¡-ì‹¤ì œ)"] / out.loc[mask, target_col] * 100).abs()

    pd.options.display.float_format = '{:,.2f}'.format
    print("\n[Sample predictions (Transformer, 4 Features)]")
    print(out.to_string())

if __name__ == "__main__":
    main()
