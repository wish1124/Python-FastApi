import numpy as np
import pandas as pd
import torch
import os
import matplotlib.pyplot as plt

# ê°™ì€ í´ë”ì— model_transformer.pyê°€ ìˆì–´ì•¼ í•¨
from model_transformer import run_training_transformer

def main():
    # ë°ì´í„° ê²½ë¡œ ì„¤ì • (ìƒëŒ€ ê²½ë¡œ ì£¼ì˜)
    csv_path = "../dataset/dataset_feature_selected.csv"
    
    if not os.path.exists(csv_path):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        # í˜¹ì‹œ í˜„ì¬ í´ë” ê¸°ì¤€ì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì²´í¬
        if os.path.exists("./dataset_feature_selected.csv"):
            csv_path = "./dataset_feature_selected.csv"
            print(f"ğŸ“‚ í˜„ì¬ í´ë”ì—ì„œ íŒŒì¼ ë°œê²¬: {csv_path}")
        else:
            return

    print(f"ğŸ“‚ Loading data from: {csv_path}")
    df = pd.read_csv(csv_path)

    # 1. íƒ€ê²Ÿ ì»¬ëŸ¼ ì •ì˜
    target_col = "ë‚™ì°°ê°€"

    # 2. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (íƒ€ê²Ÿì´ ìˆëŠ”ì§€)
    if target_col not in df.columns:
        raise KeyError(f"CSVì— íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_col}'ì´(ê°€) ì—†ìŠµë‹ˆë‹¤.")

    # 3. Feature ì»¬ëŸ¼ ìë™ ì •ì˜ (ì „ì²´ ì»¬ëŸ¼ì—ì„œ íƒ€ê²Ÿë§Œ ì œì™¸)
    feature_cols = [c for c in df.columns if c != target_col]
    print(f"ğŸ“Š ê°ì§€ëœ ì…ë ¥ í”¼ì²˜ ({len(feature_cols)}ê°œ): {feature_cols}")

    # 4. í•™ìŠµ ì‹¤í–‰
    # (ì£¼ì˜: run_training_transformer í•¨ìˆ˜ì˜ ì •ì˜ì— ì—†ëŠ” ì¸ìëŠ” ë„£ìœ¼ë©´ ì—ëŸ¬ ë‚¨)
    print("ğŸš€ í•™ìŠµ ì‹œì‘...")
    res = run_training_transformer(
        df=df,
        feature_cols=feature_cols,
        target_col=target_col,
        target_log=True,         # íƒ€ê²Ÿ ë¡œê·¸ ë³€í™˜ ì‚¬ìš©
        epochs=200,               
        patience=10,
        batch_size=64,
        lr=1e-4,
        weight_decay=1e-4,
        d_model=512,             
        nhead=4,
        num_layers=3,
        dim_feedforward=2048,     
        dropout=0.1,
        # ì•„ë˜ ì˜µì…˜ë“¤ì€ í•¨ìˆ˜ ì •ì˜ì— ë”°ë¼ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆì–´ ì œê±°í•˜ê±°ë‚˜ í™•ì¸ í•„ìš”
        # feature_noise_std=0.001, 
        verbose=True
    )

    print("\nâœ… Best VAL Loss:", res.best_val)
    print("âœ… TEST Loss:", res.test)

    # 5. ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    sample_size = min(5, len(df))
    sample = df.sample(sample_size, random_state=42).copy()

    # ë°ì´í„° ì „ì²˜ë¦¬ (Scaler ì‚¬ìš©)
    X = sample[feature_cols].to_numpy(dtype=np.float32)
    X_s = res.x_scaler.transform(X)

    # í…ì„œ ë³€í™˜ ë° 3D ë³€í™˜ (ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ)
    # model_transformer.pyì˜ forward ë©”ì„œë“œê°€ (B, F, 1) í˜•íƒœë¥¼ ì›í•¨
    x_t = torch.from_numpy(X_s)
    
    # 2D (Batch, Feature) -> 3D (Batch, Feature, 1) ë³€í™˜
    x_t = x_t.unsqueeze(-1)  # ë§ˆì§€ë§‰ ì°¨ì›ì— 1 ì¶”ê°€
    
    device = next(res.model.parameters()).device
    res.model.eval()
    
    with torch.no_grad():
        # ì´ì œ (Batch, Feature, 1) í˜•íƒœë¡œ ì „ë‹¬
        pred_s = res.model(x_t.to(device)).cpu().numpy()

    # ì—­ë³€í™˜ (Log -> ì›ë˜ ê°€ê²©)
    use_log = getattr(res, 'target_log', True)
    
    pred_log = res.y_scaler.inverse_transform(pred_s)
    pred_amt = np.expm1(pred_log) if use_log else pred_log

    # ê²°ê³¼ ë¹„êµ ì¶œë ¥
    out = sample[[target_col]].copy()
    out["ì˜ˆì¸¡ë‚™ì°°ê°€"] = pred_amt
    out["ì˜¤ì°¨(ì˜ˆì¸¡-ì‹¤ì œ)"] = out["ì˜ˆì¸¡ë‚™ì°°ê°€"] - out[target_col]
    
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    out["ì˜¤ì°¨ìœ¨(%)"] = 0.0
    mask = out[target_col] != 0
    out.loc[mask, "ì˜¤ì°¨ìœ¨(%)"] = (out.loc[mask, "ì˜¤ì°¨(ì˜ˆì¸¡-ì‹¤ì œ)"] / out.loc[mask, target_col] * 100).abs()

    pd.options.display.float_format = '{:,.2f}'.format
    print("\n[Sample predictions (Unit: KRW)]")
    print(out.to_string())


if __name__ == "__main__":
    main()
